data:
  raw_csv: data/raw/delivery_reviews.csv
  text_col_candidates: ["review", "review_text", "text", "comment"]
  rating_col_candidates: ["rating", "stars", "score"]
  id_col_candidates: ["id", "row_id"]
  test_size: 0.2
  stratify_on_rating: true

cleaning:
  lowercase: true
  remove_stopwords: true
  lemmatize: true
  remove_punctuation: true


sentiment_from_rating:
  negative_max: 2
  neutral_min: 3
  neutral_max: 3
  positive_min: 4

# --- add below your existing keys ---

eta:
  training_csv: data/processed/eta_train.csv
  model_path: models/eta_model.joblib
  metrics_path: reports/eta_metrics.json
  n_samples: 3000
  random_seed: 42

churn:
  training_csv: data/processed/churn_train.csv
  model_path: models/churn_model.joblib
  metrics_path: reports/churn_metrics.json
  n_samples: 2000
  random_seed: 42

output:
  results_dir: data/processed
  models_dir: fast_delivery_ml_project/models

transformer:
  model_name: distilbert-base-uncased
  max_length: 128
  batch_size: 32
  epochs: 3
  lr: 5e-5          
  warmup_ratio: 0.1
  
features:
  tfidf_max_features: 10000
  ngram_range: [1, 2]
  min_df: 1
  max_df: 1.0

models:
  classical:
    logistic_regression:
      C: 1.0
      max_iter: 100
    random_forest:
      n_estimators: 100
      max_depth: 10
    svm:
      C: 1.0
      kernel: linear
    linear_svc:
      C: 1.0
    xgboost:
      max_depth: 6
      learning_rate: 0.1
      n_estimators: 100
      objective: multi:softprob
    lightgbm:
      num_leaves: 31
      learning_rate: 0.05
      n_estimators: 100

  regression:
    ridge:
      alpha: 1.0
      max_iter: 1000
    random_forest:
      n_estimators: 100
    xgboost:
      n_estimators: 100
      learning_rate: 0.1
      max_depth: 6
    lightgbm:
      n_estimators: 100
      learning_rate: 0.05
      num_leaves: 31
